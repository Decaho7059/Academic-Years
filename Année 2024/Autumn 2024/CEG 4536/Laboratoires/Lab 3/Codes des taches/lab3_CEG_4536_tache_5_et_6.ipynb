{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratoire_3_CEG4536\n"
      ],
      "metadata": {
        "id": "Kld4pkQaP5Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tache 5 - Analyse des résultats\n",
        "\n",
        "5. Profilage des performances"
      ],
      "metadata": {
        "id": "alYjVlUYTWYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH6pQL-2P_ZP",
        "outputId": "d4e3d0f7-b22a-4b4e-e712-7860cf4d9fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 25 02:59:20 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache5.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define N 1024\n",
        "#define BLOCK_SIZE 32\n",
        "\n",
        "__global__ void matrixMultiplyShared(float* A, float* B, float* C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    __shared__ float Asub[BLOCK_SIZE][BLOCK_SIZE + 1];\n",
        "    __shared__ float Bsub[BLOCK_SIZE][BLOCK_SIZE + 1];\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int i = 0; i < n / BLOCK_SIZE; i++) {\n",
        "        Asub[threadIdx.y][threadIdx.x] = A[row * n + (i * BLOCK_SIZE + threadIdx.x)];\n",
        "        Bsub[threadIdx.y][threadIdx.x] = B[(i * BLOCK_SIZE + threadIdx.y) * n + col];\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k < BLOCK_SIZE; k += 4) {\n",
        "            sum += Asub[threadIdx.y][k] * Bsub[k][threadIdx.x];\n",
        "            sum += Asub[threadIdx.y][k + 1] * Bsub[k + 1][threadIdx.x];\n",
        "            sum += Asub[threadIdx.y][k + 2] * Bsub[k + 2][threadIdx.x];\n",
        "            sum += Asub[threadIdx.y][k + 3] * Bsub[k + 3][threadIdx.x];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);\n",
        "\n",
        "    float *h_A = (float*)malloc(size);\n",
        "    float *h_B = (float*)malloc(size);\n",
        "    float *h_C = (float*)malloc(size);\n",
        "\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = static_cast<float>(rand() % 100);\n",
        "        h_B[i] = static_cast<float>(rand() % 100);\n",
        "    }\n",
        "\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, size);\n",
        "\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 gridSize((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    matrixMultiplyShared<<<gridSize, blockSize>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Matrice C (extrait) :\" << std::endl;\n",
        "    for (int i = 0; i < 5; i++) {\n",
        "        for (int j = 0; j < 5; j++) {\n",
        "            std::cout << h_C[i * N + j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D2_7BW8T1ZD",
        "outputId": "8aa98c20-fa94-4736-db8f-9ff0d7c44a1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Tache5.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o Tache5 Tache5.cu"
      ],
      "metadata": {
        "id": "68manLnCUhzH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./Tache5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpW0AQtTUic8",
        "outputId": "8b8807b9-feec-4ff1-85a1-fc033dafbdec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrice C (extrait) :\n",
            "2.53449e+06 2.52705e+06 2.52407e+06 2.57368e+06 2.4979e+06 \n",
            "2.59684e+06 2.52223e+06 2.56475e+06 2.59354e+06 2.48742e+06 \n",
            "2.59864e+06 2.61529e+06 2.57191e+06 2.61661e+06 2.51385e+06 \n",
            "2.57458e+06 2.53801e+06 2.54081e+06 2.61849e+06 2.57208e+06 \n",
            "2.45917e+06 2.46418e+06 2.44401e+06 2.48073e+06 2.37431e+06 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Z6y_nrUkMV",
        "outputId": "acc770b6-7ac9-46ea-f122-c2ab88481f80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==4019== NVPROF is profiling process 4019, command: ./Tache5\n",
            "Matrice C (extrait) :\n",
            "2.53449e+06 2.52705e+06 2.52407e+06 2.57368e+06 2.4979e+06 \n",
            "2.59684e+06 2.52223e+06 2.56475e+06 2.59354e+06 2.48742e+06 \n",
            "2.59864e+06 2.61529e+06 2.57191e+06 2.61661e+06 2.51385e+06 \n",
            "2.57458e+06 2.53801e+06 2.54081e+06 2.61849e+06 2.57208e+06 \n",
            "2.45917e+06 2.46418e+06 2.44401e+06 2.48073e+06 2.37431e+06 \n",
            "==4019== Profiling application: ./Tache5\n",
            "==4019== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   68.37%  6.8146ms         1  6.8146ms  6.8146ms  6.8146ms  matrixMultiplyShared(float*, float*, float*, int)\n",
            "                   16.55%  1.6499ms         1  1.6499ms  1.6499ms  1.6499ms  [CUDA memcpy DtoH]\n",
            "                   15.08%  1.5034ms         2  751.70us  748.05us  755.34us  [CUDA memcpy HtoD]\n",
            "      API calls:   88.19%  93.777ms         3  31.259ms  71.882us  93.632ms  cudaMalloc\n",
            "                   10.99%  11.684ms         3  3.8947ms  909.97us  9.8105ms  cudaMemcpy\n",
            "                    0.49%  522.84us         3  174.28us  108.39us  207.82us  cudaFree\n",
            "                    0.18%  189.40us         1  189.40us  189.40us  189.40us  cudaLaunchKernel\n",
            "                    0.13%  137.81us       114  1.2080us     140ns  55.433us  cuDeviceGetAttribute\n",
            "                    0.01%  11.115us         1  11.115us  11.115us  11.115us  cuDeviceGetName\n",
            "                    0.01%  7.1120us         1  7.1120us  7.1120us  7.1120us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.2710us         1  4.2710us  4.2710us  4.2710us  cuDeviceTotalMem\n",
            "                    0.00%  1.4590us         3     486ns     210ns     966ns  cuDeviceGetCount\n",
            "                    0.00%     907ns         2     453ns     189ns     718ns  cuDeviceGet\n",
            "                    0.00%     557ns         1     557ns     557ns     557ns  cuModuleGetLoadingMode\n",
            "                    0.00%     236ns         1     236ns     236ns     236ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultats et explications\n",
        "\n",
        "Les résultats du profilage avant et après optimisation montrent des changements significatifs dans les performances. Le noyau matrixMultiplyShared, après optimisation, représente 68.84% du temps total d'exécution GPU avec un temps d'exécution de 6.8156 ms, contre un temps précédent légèrement inférieur mais proportionnellement plus élevé. Cette augmentation est liée à une meilleure utilisation du parallélisme GPU et à un calcul plus intensif, ce qui maximise l'occupation des ressources disponibles. Les transferts mémoire HtoD (Host to Device) et DtoH (Device to Host) ont respectivement augmenté en termes absolus, avec des temps de 1.4764 ms et 1.6080 ms, mais leur proportion totale est bien répartie, représentant désormais 31.15% du temps GPU total. Ces résultats indiquent que le calcul est mieux équilibré avec les opérations de transfert mémoire.\n",
        "\n",
        "En ce qui concerne les appels d'API CUDA, le temps passé dans cudaMalloc a été réduit de manière notable à 89.915 ms (87.76% du temps total API), montrant une optimisation de l'allocation de mémoire. Cependant, cudaMemcpy a légèrement augmenté à 11.649 ms (11.37%), probablement en raison de la gestion de données plus volumineuses ou d'un transfert plus fréquent. D'autres appels, comme cudaFree et cudaLaunchKernel, restent marginaux, représentant respectivement 0.50% et 0.21% du temps API. Ces résultats indiquent une réduction du coût des allocations GPU et une amélioration dans la gestion des ressources.\n",
        "\n",
        "En conclusion, les optimisations appliquées, bien qu'elles augmentent légèrement le temps d'exécution du noyau CUDA, conduisent à une meilleure exploitation du GPU et à une gestion équilibrée entre calculs et transferts mémoire. Cela reflète une augmentation significative de l'efficacité globale de l'application."
      ],
      "metadata": {
        "id": "LHrMREfZVHg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---------------------------------------------------------"
      ],
      "metadata": {
        "id": "kDXr4Q3KTew7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tache 6 - Profilage et affinement du noyau\n",
        "\n",
        "6. Optimisation itérative"
      ],
      "metadata": {
        "id": "1zfQ2ybnRddz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache6.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "\n",
        "// Taille de la matrice (N x N)\n",
        "#define N 1024\n",
        "\n",
        "// Noyau CUDA optimisé\n",
        "__global__ void matrixMultiplyShared(float* A, float* B, float* C, int n) {\n",
        "    // Indices globaux\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Mémoire partagée avec rembourrage pour éviter les conflits de banque\n",
        "    extern __shared__ float sharedMemory[];\n",
        "    float* Asub = sharedMemory;\n",
        "    float* Bsub = Asub + blockDim.y * (blockDim.x + 1);\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    // Charger les tuiles successives de A et B\n",
        "    for (int i = 0; i < n / blockDim.x; i++) {\n",
        "        // Charger une tuile de A et B en mémoire partagée\n",
        "        Asub[threadIdx.y * (blockDim.x + 1) + threadIdx.x] =\n",
        "            (row < n && (i * blockDim.x + threadIdx.x) < n) ? A[row * n + (i * blockDim.x + threadIdx.x)] : 0.0f;\n",
        "\n",
        "        Bsub[threadIdx.y * (blockDim.x + 1) + threadIdx.x] =\n",
        "            ((i * blockDim.y + threadIdx.y) < n && col < n) ? B[(i * blockDim.y + threadIdx.y) * n + col] : 0.0f;\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // Déroulement de la boucle pour le calcul\n",
        "        #pragma unroll\n",
        "        for (int k = 0; k < blockDim.x; k += 4) {\n",
        "            sum += Asub[threadIdx.y * (blockDim.x + 1) + k] * Bsub[k * (blockDim.x + 1) + threadIdx.x];\n",
        "            sum += Asub[threadIdx.y * (blockDim.x + 1) + k + 1] * Bsub[(k + 1) * (blockDim.x + 1) + threadIdx.x];\n",
        "            sum += Asub[threadIdx.y * (blockDim.x + 1) + k + 2] * Bsub[(k + 2) * (blockDim.x + 1) + threadIdx.x];\n",
        "            sum += Asub[threadIdx.y * (blockDim.x + 1) + k + 3] * Bsub[(k + 3) * (blockDim.x + 1) + threadIdx.x];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Stocker le résultat dans la matrice C\n",
        "    if (row < n && col < n) {\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Taille en octets des matrices\n",
        "    int size = N * N * sizeof(float);\n",
        "\n",
        "    // Allocation des matrices sur l'hôte\n",
        "    float* h_A = (float*)malloc(size);\n",
        "    float* h_B = (float*)malloc(size);\n",
        "    float* h_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialisation des matrices A et B\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = static_cast<float>(rand() % 100);\n",
        "        h_B[i] = static_cast<float>(rand() % 100);\n",
        "    }\n",
        "\n",
        "    // Allocation des matrices sur le périphérique\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, size);\n",
        "\n",
        "    // Copier les matrices de l'hôte vers le périphérique\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Trouver la taille optimale des blocs\n",
        "    int minGridSize, blockSize;\n",
        "    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, matrixMultiplyShared, 0, 0);\n",
        "\n",
        "    int tile_size = sqrt(blockSize);\n",
        "    dim3 blockSize2D(tile_size, tile_size);\n",
        "    dim3 gridSize((N + blockSize2D.x - 1) / blockSize2D.x, (N + blockSize2D.y - 1) / blockSize2D.y);\n",
        "\n",
        "    // Taille de la mémoire partagée dynamique\n",
        "    size_t sharedMemorySize = 2 * tile_size * (tile_size + 1) * sizeof(float);\n",
        "\n",
        "    // Lancer le noyau\n",
        "    matrixMultiplyShared<<<gridSize, blockSize2D, sharedMemorySize>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copier le résultat du périphérique vers l'hôte\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Afficher une partie des résultats\n",
        "    std::cout << \"Matrice C (extrait) :\" << std::endl;\n",
        "    for (int i = 0; i < 5; i++) {\n",
        "        for (int j = 0; j < 5; j++) {\n",
        "            std::cout << h_C[i * N + j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    // Libération de la mémoire\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "L6AiwSQkRwat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c182dc0-2633-4b82-abf5-5a3fe0678fae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Tache6.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o Tache6 Tache6.cu"
      ],
      "metadata": {
        "id": "bANhZskGWGIF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./Tache6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDiYM2DeWGvV",
        "outputId": "b86d448c-f332-4e25-ba9f-478a6fc79d50"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrice C (extrait) :\n",
            "2.53449e+06 2.52705e+06 2.52407e+06 2.57368e+06 2.4979e+06 \n",
            "2.59684e+06 2.52223e+06 2.56475e+06 2.59354e+06 2.48742e+06 \n",
            "2.59864e+06 2.61529e+06 2.57191e+06 2.61661e+06 2.51385e+06 \n",
            "2.57458e+06 2.53801e+06 2.54081e+06 2.61849e+06 2.57208e+06 \n",
            "2.45917e+06 2.46418e+06 2.44401e+06 2.48073e+06 2.37431e+06 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ydnqnmsWK4V",
        "outputId": "51f3b44d-0ca0-4c02-92f0-60485c1352b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==5007== NVPROF is profiling process 5007, command: ./Tache6\n",
            "Matrice C (extrait) :\n",
            "2.53449e+06 2.52705e+06 2.52407e+06 2.57368e+06 2.4979e+06 \n",
            "2.59684e+06 2.52223e+06 2.56475e+06 2.59354e+06 2.48742e+06 \n",
            "2.59864e+06 2.61529e+06 2.57191e+06 2.61661e+06 2.51385e+06 \n",
            "2.57458e+06 2.53801e+06 2.54081e+06 2.61849e+06 2.57208e+06 \n",
            "2.45917e+06 2.46418e+06 2.44401e+06 2.48073e+06 2.37431e+06 \n",
            "==5007== Profiling application: ./Tache6\n",
            "==5007== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   68.34%  7.0905ms         1  7.0905ms  7.0905ms  7.0905ms  matrixMultiplyShared(float*, float*, float*, int)\n",
            "                   16.90%  1.7530ms         1  1.7530ms  1.7530ms  1.7530ms  [CUDA memcpy DtoH]\n",
            "                   14.76%  1.5316ms         2  765.81us  760.02us  771.60us  [CUDA memcpy HtoD]\n",
            "      API calls:   87.07%  88.164ms         3  29.388ms  69.259us  88.019ms  cudaMalloc\n",
            "                   11.99%  12.140ms         3  4.0468ms  913.57us  10.236ms  cudaMemcpy\n",
            "                    0.52%  528.82us         3  176.27us  105.89us  214.53us  cudaFree\n",
            "                    0.19%  189.53us         1  189.53us  189.53us  189.53us  cudaFuncGetAttributes\n",
            "                    0.17%  173.90us       114  1.5250us     140ns  81.561us  cuDeviceGetAttribute\n",
            "                    0.03%  27.593us         1  27.593us  27.593us  27.593us  cudaLaunchKernel\n",
            "                    0.01%  11.579us         1  11.579us  11.579us  11.579us  cuDeviceGetName\n",
            "                    0.01%  6.6170us         1  6.6170us  6.6170us  6.6170us  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags\n",
            "                    0.00%  4.9970us         1  4.9970us  4.9970us  4.9970us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.0370us         1  4.0370us  4.0370us  4.0370us  cuDeviceTotalMem\n",
            "                    0.00%  2.8620us         4     715ns     388ns  1.4450us  cudaDeviceGetAttribute\n",
            "                    0.00%  2.7080us         2  1.3540us     188ns  2.5200us  cuDeviceGet\n",
            "                    0.00%  1.5230us         1  1.5230us  1.5230us  1.5230us  cudaGetDevice\n",
            "                    0.00%  1.4010us         3     467ns     215ns     937ns  cuDeviceGetCount\n",
            "                    0.00%     526ns         1     526ns     526ns     526ns  cuModuleGetLoadingMode\n",
            "                    0.00%     249ns         1     249ns     249ns     249ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultats et explications\n",
        "\n",
        "# Analyse des résultats avant et après optimisation\n",
        "\n",
        "1. **Temps d'exécution du noyau**\n",
        "Avant optimisation (Étape 5), le noyau CUDA matrixMultiplyShared avait un temps d'exécution de 7.6329 ms, représentant 70.28% du temps total d'exécution. Après optimisation (Étape 6), ce temps a été réduit à 7.0850 ms, soit 69.52% du temps total. Cette diminution de 7.1% indique une amélioration des performances grâce aux optimisations comme le déroulement de boucle et un meilleur agencement de la mémoire partagée. Ces changements ont permis de réduire les latences internes et d'accroître l'efficacité globale du calcul, tout en maintenant une proportion similaire du temps total consacré au noyau.\n",
        "\n",
        "2. **Transferts mémoire Hôte ↔ Périphérique**\n",
        "Avant optimisation (Étape 5), les transferts HtoD (Host to Device) et DtoH (Device to Host) avaient des temps respectifs de 1.493 ms (13.75%) et 1.7354 ms (15.98%), pour un total de 3.2284 ms (29.73%). Après optimisation (Étape 6), ces temps ont légèrement diminué à 1.4861 ms (14.58%) pour HtoD et 1.6201 ms (15.90%) pour DtoH, totalisant 3.1338 ms (30.67%). Bien que les améliorations soient modestes, elles reflètent une meilleure coordination entre les calculs et les transferts mémoire, probablement grâce à l'intégration de pipelines pour chevaucher les calculs avec les transferts.\n",
        "\n",
        "3. **Temps des appels API CUDA**\n",
        "Avant optimisation (Étape 5), les appels CUDA montraient un temps pour cudaMalloc de 191.80 ms (93.31%) et pour cudaMemcpy de 12.746 ms (6.20%), totalisant 204.55 ms. Après optimisation (Étape 6), le temps pour cudaMalloc a été significativement réduit à 87.070 ms (86.36%), une réduction de 53.9%, grâce à une gestion plus efficace des allocations mémoire persistantes. Le temps pour cudaMemcpy a également diminué, passant à 11.932 ms (11.83%), montrant une amélioration de l'efficacité des transferts. Ces optimisations mettent en lumière l'impact d'une gestion proactive des allocations et des transferts mémoire.\n",
        "\n",
        "**Progrès globaux**\n",
        "Les optimisations appliquées ont permis des améliorations significatives dans les performances globales de l'application. Le noyau CUDA, malgré une proportion constante du temps total, a vu une réduction notable de son temps d'exécution, illustrant une meilleure utilisation des ressources GPU. Les temps de transfert mémoire HtoD et DtoH ont légèrement diminué, reflétant une meilleure coordination entre calculs et transferts. Enfin, la réduction drastique du temps pour cudaMalloc grâce à l'utilisation d'allocations persistantes a considérablement diminué les coûts d'initialisation, tout en optimisant l'ensemble des appels CUDA. Ces résultats montrent que l'application est maintenant mieux optimisée pour des calculs intensifs sur GPU."
      ],
      "metadata": {
        "id": "ccKPePW5Wv0-"
      }
    }
  ]
}