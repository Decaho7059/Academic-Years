{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratoire 3 _ CEG 4536\n"
      ],
      "metadata": {
        "id": "0ig2jqLrJgSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tache 3 - Implémentation pratique\n",
        "\n",
        "3. Utilisation de la mémoire partagée"
      ],
      "metadata": {
        "id": "ikI-I0EcJvvV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXB8WLRrJfZf",
        "outputId": "ecbb69ee-0b7b-4192-8a12-5cab8101fb70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 25 02:31:14 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache3.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 1024  // Taille de la matrice\n",
        "#define TILE_SIZE 32  // Taille des tuiles pour la mémoire partagée\n",
        "\n",
        "// Noyau CUDA pour la multiplication matricielle avec mémoire partagée\n",
        "__global__ void matrixMultiplyShared(float *a, float *b, float *c, int n) {\n",
        "    __shared__ float tileA[TILE_SIZE][TILE_SIZE];\n",
        "    __shared__ float tileB[TILE_SIZE][TILE_SIZE];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Calcul de la multiplication matricielle\n",
        "    float sum = 0.0f;\n",
        "    for (int t = 0; t < (n + TILE_SIZE - 1) / TILE_SIZE; t++) {\n",
        "      // Chargement des tuiles de A et B dans la mémoire partagée\n",
        "      if (row < n && t * TILE_SIZE + threadIdx.x < n) {\n",
        "            tileA[threadIdx.y][threadIdx.x] = a[row * n + t * TILE_SIZE + threadIdx.x];\n",
        "        } else {\n",
        "            tileA[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "        }\n",
        "\n",
        "        if (col < n && t * TILE_SIZE + threadIdx.y < n) {\n",
        "            tileB[threadIdx.y][threadIdx.x] = b[(t * TILE_SIZE + threadIdx.y) * n + col];\n",
        "        } else {\n",
        "            tileB[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // Calculer la multiplication de la tuile\n",
        "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
        "            sum += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Ecrire le résultat dans C\n",
        "    if (row < n && col < n) {\n",
        "        c[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int size = N * N * sizeof(float);\n",
        "    float *h_A, *h_B, *h_C;\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocation de mémoire sur l'hôte\n",
        "    h_A = (float*)malloc(size);\n",
        "    h_B = (float*)malloc(size);\n",
        "    h_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialisation des matrices A et B\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = rand() % 100 / 10.0f;  // Valeurs aléatoires entre 0 et 10\n",
        "        h_B[i] = rand() % 100 / 10.0f;\n",
        "    }\n",
        "\n",
        "    // Allocation de mémoire sur le GPU\n",
        "    cudaMalloc((void**)&d_A, size);\n",
        "    cudaMalloc((void**)&d_B, size);\n",
        "    cudaMalloc((void**)&d_C, size);\n",
        "\n",
        "    // Copie des matrices de l'hôte au GPU\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Lancement du noyau CUDA\n",
        "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
        "    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
        "    matrixMultiplyShared<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copie des résultats du GPU vers l'hôte\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Affichage d'une partie des résultats\n",
        "    printf(\"Matrice C (extrait) :\\n\");\n",
        "    for (int i = 0; i < 5; i++) {\n",
        "        for (int j = 0; j < 5; j++) {\n",
        "            printf(\"%0.2f \", h_C[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Libération de la mémoire\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY4zxJbdJ_Hd",
        "outputId": "9c254c83-ff0a-40e0-91d2-ddee257f07a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Tache3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc Tache3.cu -o Tache3"
      ],
      "metadata": {
        "id": "hMU1PkSdKRCm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./Tache3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GjfNKUPKTpG",
        "outputId": "85ba6287-e35e-43de-c27d-4c5f958d7f51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrice C (extrait) :\n",
            "25344.94 25270.46 25240.65 25736.77 24979.00 \n",
            "25968.35 25222.31 25647.50 25935.41 24874.16 \n",
            "25986.41 26152.93 25719.11 26166.12 25138.52 \n",
            "25745.83 25380.14 25408.12 26184.89 25720.78 \n",
            "24591.71 24641.83 24440.13 24807.35 23743.05 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYiO8yp4KVOO",
        "outputId": "d463a85a-d7f0-4ae4-deba-06a9f9e87126"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2325== NVPROF is profiling process 2325, command: ./Tache3\n",
            "Matrice C (extrait) :\n",
            "25344.94 25270.46 25240.65 25736.77 24979.00 \n",
            "25968.35 25222.31 25647.50 25935.41 24874.16 \n",
            "25986.41 26152.93 25719.11 26166.12 25138.52 \n",
            "25745.83 25380.14 25408.12 26184.89 25720.78 \n",
            "24591.71 24641.83 24440.13 24807.35 23743.05 \n",
            "==2325== Profiling application: ./Tache3\n",
            "==2325== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   58.54%  5.3674ms         1  5.3674ms  5.3674ms  5.3674ms  matrixMultiplyShared(float*, float*, float*, int)\n",
            "                   22.65%  2.0765ms         1  2.0765ms  2.0765ms  2.0765ms  [CUDA memcpy DtoH]\n",
            "                   18.81%  1.7247ms         2  862.37us  859.33us  865.41us  [CUDA memcpy HtoD]\n",
            "      API calls:   94.37%  203.25ms         3  67.751ms  68.098us  203.11ms  cudaMalloc\n",
            "                    5.16%  11.106ms         3  3.7021ms  1.0783ms  8.8884ms  cudaMemcpy\n",
            "                    0.25%  548.68us         3  182.89us  143.09us  211.67us  cudaFree\n",
            "                    0.13%  280.78us         1  280.78us  280.78us  280.78us  cudaLaunchKernel\n",
            "                    0.07%  146.52us       114  1.2850us     156ns  57.715us  cuDeviceGetAttribute\n",
            "                    0.01%  23.236us         1  23.236us  23.236us  23.236us  cuDeviceGetPCIBusId\n",
            "                    0.01%  13.276us         1  13.276us  13.276us  13.276us  cuDeviceGetName\n",
            "                    0.00%  5.5130us         1  5.5130us  5.5130us  5.5130us  cuDeviceTotalMem\n",
            "                    0.00%  1.5710us         3     523ns     203ns  1.0710us  cuDeviceGetCount\n",
            "                    0.00%  1.0770us         2     538ns     200ns     877ns  cuDeviceGet\n",
            "                    0.00%     604ns         1     604ns     604ns     604ns  cuModuleGetLoadingMode\n",
            "                    0.00%     278ns         1     278ns     278ns     278ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultat et explication\n",
        "\n",
        "Les résultats du profilage montrent que l'optimisation de la multiplication matricielle à l'aide de la mémoire partagée dans le noyau matrixMultiplyShared a considérablement amélioré les performances. Le noyau CUDA représente 58.54% du temps total d'exécution GPU, avec un temps d'exécution de 5.3674 ms, confirmant une accélération significative par rapport aux implémentations précédentes utilisant uniquement la mémoire globale.\n",
        "Les transferts de données entre l'hôte et le périphérique (HtoD) ont consommé 18.81% du temps total, soit 1.7247 ms répartis sur deux appels, tandis que les transferts du périphérique vers l'hôte (DtoH) ont occupé 22.65%, soit 2.0765 ms pour un seul appel.\n",
        "Ces transferts restent un goulot d'étranglement malgré l'amélioration des calculs.\n",
        "Du côté des appels d'API CUDA, cudaMalloc domine avec 94.37% du temps total API (203.25 ms répartis sur trois appels), ce qui reflète le coût élevé des allocations de mémoire sur le GPU. Les appels cudaMemcpy représentent 5.16% (11.106 ms pour trois appels), montrant que la gestion des transferts pourrait encore être optimisée.\n",
        "Les autres appels, comme cudaFree et cudaLaunchKernel, ne représentent qu'une fraction négligeable du temps total, témoignant d'une gestion efficace en dehors des principales opérations.\n",
        "\n",
        "En conclusion, l'utilisation de la mémoire partagée dans cette implémentation a réduit de manière significative le temps d'exécution du calcul principal, mais les transferts mémoire et l'allocation GPU demeurent des axes d'amélioration potentiels pour optimiser davantage les performances globales."
      ],
      "metadata": {
        "id": "Z4kwT6uiLkBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "03E6Qn7MPuYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -------------------------------------------------\n"
      ],
      "metadata": {
        "id": "GZz8rmw5Pv9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tache 4 - Profilage et évaluation\n",
        "\n",
        "4. Optimiser les schémas d'accès à la mémoire partagée"
      ],
      "metadata": {
        "id": "ttWuLIvYPxzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache4.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define BLOCK_SIZE 32\n",
        "#define PADDING 1 // pour eviter les blocs trop petits\n",
        "\n",
        "__global__ void matrixMultiplyShared(float* A, float* B, float* C, int N) {\n",
        "    __shared__ float tileA[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ float tileB[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int m = 0; m < (N + BLOCK_SIZE - 1) / BLOCK_SIZE; ++m) {\n",
        "        if (row < N && (m * BLOCK_SIZE + threadIdx.x) < N)\n",
        "            tileA[threadIdx.y][threadIdx.x] = A[row * N + (m * BLOCK_SIZE + threadIdx.x)];\n",
        "        else\n",
        "            tileA[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        if (col < N && (m * BLOCK_SIZE + threadIdx.y) < N)\n",
        "            tileB[threadIdx.y][threadIdx.x] = B[(m * BLOCK_SIZE + threadIdx.y) * N + col];\n",
        "        else\n",
        "            tileB[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k < BLOCK_SIZE; ++k) {\n",
        "            sum += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (row < N && col < N)\n",
        "        C[row * N + col] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 512;  // Taille de la matrice, modifiable selon les besoins\n",
        "    int size = N * N * sizeof(float);\n",
        "    float *h_A, *h_B, *h_C;\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocation de mémoire sur l'hôte\n",
        "    h_A = (float*)malloc(size);\n",
        "    h_B = (float*)malloc(size);\n",
        "    h_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialisation des matrices A et B aléatoires\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = rand() % 100 / 10.0;\n",
        "        h_B[i] = rand() % 100 / 10.0;\n",
        "    }\n",
        "\n",
        "    // Allocation de mémoire sur le GPU\n",
        "    cudaMalloc((void**)&d_A, size);\n",
        "    cudaMalloc((void**)&d_B, size);\n",
        "    cudaMalloc((void**)&d_C, size);\n",
        "\n",
        "    // Copie des matrices de l'hôte au GPU\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) { //pour s'assurer que cudaMempcy copie bien les données entre l'hote et le gpu\n",
        "      printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "      }\n",
        "\n",
        "    // Lancement du noyau CUDA\n",
        "    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 numBlocks((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "    matrixMultiplyShared<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copie des résultats du GPU vers l'hôte\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Affichage d'une partie des résultats\n",
        "    printf(\"Matrice C (extrait) :\\n\");\n",
        "    for (int i = 0; i < 5; i++) {\n",
        "        for (int j = 0; j < 5; j++) {\n",
        "            printf(\"%0.2f \", h_C[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Libération de la mémoire\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MzcshOJQCA6",
        "outputId": "42fc50de-de12-4553-a1e5-0ffe25c2c908"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Tache4.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc Tache4.cu -o Tache4"
      ],
      "metadata": {
        "id": "5z2VIQ-8QQlK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./Tache4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJr9ilsNQSSi",
        "outputId": "c45a62b9-6435-4ba8-b2ca-8cf984d1bc32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrice C (extrait) :\n",
            "12815.51 12810.97 13229.42 12867.26 13151.88 \n",
            "12641.00 12111.48 12917.40 12138.28 13090.13 \n",
            "12931.40 12732.82 13043.32 12652.42 13247.57 \n",
            "12449.74 12155.74 12686.56 12167.19 12716.53 \n",
            "13361.70 13210.03 13280.66 12809.75 13371.86 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71duKtgpQTrW",
        "outputId": "fea3b444-0f1a-475d-93c4-a62b4ae28adc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==7913== NVPROF is profiling process 7913, command: ./Tache4\n",
            "Matrice C (extrait) :\n",
            "12815.51 12810.97 13229.42 12867.26 13151.88 \n",
            "12641.00 12111.48 12917.40 12138.28 13090.13 \n",
            "12931.40 12732.82 13043.32 12652.42 13247.57 \n",
            "12449.74 12155.74 12686.56 12167.19 12716.53 \n",
            "13361.70 13210.03 13280.66 12809.75 13371.86 \n",
            "==7913== Profiling application: ./Tache4\n",
            "==7913== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   72.92%  716.22us         1  716.22us  716.22us  716.22us  matrixMultiplyShared(float*, float*, float*, int)\n",
            "                   17.93%  176.13us         2  88.064us  87.584us  88.544us  [CUDA memcpy HtoD]\n",
            "                    9.14%  89.792us         1  89.792us  89.792us  89.792us  [CUDA memcpy DtoH]\n",
            "      API calls:   98.51%  189.58ms         3  63.193ms  3.5900us  189.50ms  cudaMalloc\n",
            "                    1.14%  2.1858ms         3  728.59us  259.41us  1.6105ms  cudaMemcpy\n",
            "                    0.12%  237.05us         3  79.016us  14.374us  116.84us  cudaFree\n",
            "                    0.12%  222.52us         1  222.52us  222.52us  222.52us  cudaLaunchKernel\n",
            "                    0.10%  186.17us       114  1.6330us     210ns  70.335us  cuDeviceGetAttribute\n",
            "                    0.01%  16.982us         1  16.982us  16.982us  16.982us  cuDeviceGetName\n",
            "                    0.00%  6.7450us         1  6.7450us  6.7450us  6.7450us  cuDeviceTotalMem\n",
            "                    0.00%  6.0580us         1  6.0580us  6.0580us  6.0580us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.3360us         3  1.1120us     347ns  2.6180us  cuDeviceGetCount\n",
            "                    0.00%  1.8570us         2     928ns     252ns  1.6050us  cuDeviceGet\n",
            "                    0.00%     910ns         1     910ns     910ns     910ns  cudaGetLastError\n",
            "                    0.00%     716ns         1     716ns     716ns     716ns  cuModuleGetLoadingMode\n",
            "                    0.00%     522ns         1     522ns     522ns     522ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultats et explications\n",
        "\n",
        "\n",
        "Les résultats du profilage pour l'exécution de la tâche matrixMultiplyShared montrent des performances optimisées. Le noyau CUDA représente 72.92% du temps total d'exécution GPU, avec un temps d'exécution de 716.22 µs, ce qui illustre une grande efficacité dans l'utilisation de la mémoire partagée. Les transferts mémoire entre l'hôte et le périphérique (HtoD) ont occupé 17.93% du temps total, soit 176.13 µs répartis sur deux appels, tandis que les transferts du périphérique vers l'hôte (DtoH) représentent 9.14%, avec un temps de 89.792 µs pour un appel unique. Ces transferts, bien que significatifs, montrent une amélioration notable par rapport aux implémentations précédentes.\n",
        "\n",
        "Du côté des appels d'API CUDA, cudaMalloc domine toujours, représentant 98.51% du temps total API (189.58 ms répartis sur trois appels), ce qui reflète l'impact des allocations mémoire sur les performances globales. Les appels cudaMemcpy représentent 1.14% du temps total API (2.1858 ms pour trois appels), tandis que les autres appels, comme cudaFree et cudaLaunchKernel, occupent une part négligeable, avec respectivement 0.12% et 0.12% du temps total API. Cela témoigne d'une gestion efficace des ressources hors des opérations principales.\n",
        "\n",
        "En conclusion, l'utilisation de la mémoire partagée dans cette tâche a permis de réduire significativement le temps d'exécution du calcul principal à 716.22 µs, mais les transferts mémoire et les allocations GPU restent des points à optimiser davantage. Ces résultats montrent que la stratégie d'optimisation actuelle est efficace pour améliorer les performances globales."
      ],
      "metadata": {
        "id": "IRsMbJN5QUfj"
      }
    }
  ]
}