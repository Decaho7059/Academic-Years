{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratoire 4 _ CEG 4536"
      ],
      "metadata": {
        "id": "B7-8tXPcQxGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8z-fG2ZQOMs",
        "outputId": "60121373-5a96-42b4-f1fc-30115bcc433e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec  3 21:02:51 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tache 1 - Mémoire partagée\n",
        "\n"
      ],
      "metadata": {
        "id": "DISwWIPZReel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache1.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "// Taille du bloc\n",
        "#define BLOCK_SIZE 32\n",
        "\n",
        "// Kernel de transposition avec mémoire partagée et padding\n",
        "__global__ void transposeWithSharedMemory(float *input, float *output, int width, int height) {\n",
        "    __shared__ float tile[BLOCK_SIZE][BLOCK_SIZE + 1]; // +1 pour éviter les conflits de banque\n",
        "\n",
        "    int x = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n",
        "    int y = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    if (x < width && y < height) {\n",
        "        tile[threadIdx.y][threadIdx.x] = input[y * width + x];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    int transposedX = blockIdx.y * BLOCK_SIZE + threadIdx.x;\n",
        "    int transposedY = blockIdx.x * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    if (transposedX < height && transposedY < width) {\n",
        "        output[transposedY * height + transposedX] = tile[threadIdx.x][threadIdx.y];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Dimensions de la matrice\n",
        "    int width = 1024;\n",
        "    int height = 1024;\n",
        "\n",
        "    size_t size = width * height * sizeof(float);\n",
        "\n",
        "    // Allocation de mémoire hôte\n",
        "    float *h_input = (float *)malloc(size);\n",
        "    float *h_output = (float *)malloc(size);\n",
        "\n",
        "    // Initialisation de la matrice\n",
        "    for (int i = 0; i < width * height; i++) {\n",
        "        h_input[i] = static_cast<float>(i);\n",
        "    }\n",
        "\n",
        "    // Allocation de mémoire GPU\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, size);\n",
        "    cudaMalloc(&d_output, size);\n",
        "\n",
        "    // Copie des données vers le GPU\n",
        "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Configuration des dimensions du kernel\n",
        "    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 gridDim((width + BLOCK_SIZE - 1) / BLOCK_SIZE, (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    // Lancement du kernel\n",
        "    transposeWithSharedMemory<<<gridDim, blockDim>>>(d_input, d_output, width, height);\n",
        "\n",
        "    // Copie des résultats vers l'hôte\n",
        "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Nettoyage et liberation de la mamoire\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXXAp-seRsI2",
        "outputId": "fac64baf-b29e-4a82-de3a-f957cfb84bbc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Tache1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc Tache1.cu -o Tache1"
      ],
      "metadata": {
        "id": "I81WkSd-SrSb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./Tache1"
      ],
      "metadata": {
        "id": "xIFLXe1YSr3W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6O24y1NStdY",
        "outputId": "46ee3068-7eac-4644-d85d-8cba442ea108"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==8020== NVPROF is profiling process 8020, command: ./Tache1\n",
            "==8020== Profiling application: ./Tache1\n",
            "==8020== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   66.31%  1.6614ms         1  1.6614ms  1.6614ms  1.6614ms  [CUDA memcpy DtoH]\n",
            "                   31.35%  785.44us         1  785.44us  785.44us  785.44us  [CUDA memcpy HtoD]\n",
            "                    2.34%  58.688us         1  58.688us  58.688us  58.688us  transposeWithSharedMemory(float*, float*, int, int)\n",
            "      API calls:   97.53%  189.43ms         2  94.713ms  74.266us  189.35ms  cudaMalloc\n",
            "                    2.09%  4.0564ms         2  2.0282ms  950.42us  3.1060ms  cudaMemcpy\n",
            "                    0.16%  307.07us         2  153.54us  103.80us  203.28us  cudaFree\n",
            "                    0.11%  204.58us         1  204.58us  204.58us  204.58us  cudaLaunchKernel\n",
            "                    0.10%  195.52us       114  1.7150us     151ns  71.307us  cuDeviceGetAttribute\n",
            "                    0.01%  13.112us         1  13.112us  13.112us  13.112us  cuDeviceGetName\n",
            "                    0.00%  7.2880us         1  7.2880us  7.2880us  7.2880us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.1730us         1  6.1730us  6.1730us  6.1730us  cuDeviceTotalMem\n",
            "                    0.00%  1.7010us         3     567ns     256ns  1.0660us  cuDeviceGetCount\n",
            "                    0.00%  1.1450us         2     572ns     303ns     842ns  cuDeviceGet\n",
            "                    0.00%     615ns         1     615ns     615ns     615ns  cuModuleGetLoadingMode\n",
            "                    0.00%     250ns         1     250ns     250ns     250ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumé et explication\n",
        "\n",
        "Le profilage du kernel montre que les copies de données entre l’hôte et le dispositif GPU dominent le temps d’exécution total, représentant 66,31% (DtoH) et 31,35% (HtoD) du temps GPU. Cela indique que l’efficacité globale est limitée par la latence de transfert mémoire, ce qui est typique pour des opérations impliquant de grandes quantités de données. Le kernel transposeWithSharedMemory, bien qu’optimisé avec de la mémoire partagée et un padding pour éviter les conflits de banque, ne représente que 2,34% du temps total GPU. Cela suggère que les optimisations sur les accès mémoire dans le kernel sont efficaces, mais leur impact est masqué par les coûts de transfert.\n",
        "\n",
        "Le temps API est majoritairement consacré à cudaMalloc (97,53%), ce qui est attendu pour l'allocation initiale de mémoire GPU. Les temps associés à cudaMemcpy et cudaFree sont faibles par rapport aux transferts eux-mêmes, montrant une gestion mémoire correcte. L'ajout de padding (+1 dans la mémoire partagée) semble efficace pour éviter les conflits de banque, mais des améliorations peuvent encore être explorées, comme la réduction des transferts mémoire en regroupant plusieurs opérations dans une seule passe ou en utilisant des outils avancés pour des schémas d'accès encore plus efficaces."
      ],
      "metadata": {
        "id": "GiyG9fugSz-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "oMXn9Q_-WbNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "dhtkJYe5Weic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tache 2 - Optimisation du Kernel\n",
        "\n",
        "Reduction parallèle avec mémoire partagée et shuffle"
      ],
      "metadata": {
        "id": "-SZsaOz-WDM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache2.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void reduceOptimized(const float *input, float *output, int size) {\n",
        "    __shared__ float sharedData[BLOCK_SIZE];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int globalIdx = blockIdx.x * blockDim.x * 2 + threadIdx.x;\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    if (globalIdx < size) {\n",
        "        sum += input[globalIdx];\n",
        "        if (globalIdx + blockDim.x < size) {\n",
        "            sum += input[globalIdx + blockDim.x];\n",
        "        }\n",
        "    }\n",
        "    sharedData[tid] = sum;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 32; stride /= 2) {\n",
        "        if (tid < stride) {\n",
        "            sharedData[tid] += sharedData[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid < 32) {\n",
        "        sum = sharedData[tid];\n",
        "        #pragma unroll\n",
        "        for (int offset = 16; offset > 0; offset /= 2) {\n",
        "            sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);\n",
        "        }\n",
        "        if (tid == 0) {\n",
        "            sharedData[0] = sum;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = 1 << 20;  // 1 million d'éléments\n",
        "    size_t bytes = size * sizeof(float);\n",
        "\n",
        "    float *h_input = (float *)malloc(bytes);\n",
        "    float *h_output;\n",
        "\n",
        "    // Initialiser les données\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        h_input[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, bytes);\n",
        "\n",
        "    int threads = BLOCK_SIZE;\n",
        "    int blocks = (size + threads * 2 - 1) / (threads * 2);\n",
        "\n",
        "    h_output = (float *)malloc(blocks * sizeof(float));\n",
        "    cudaMalloc(&d_output, blocks * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    reduceOptimized<<<blocks, threads>>>(d_input, d_output, size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_output, d_output, blocks * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float finalResult = 0.0f;\n",
        "    for (int i = 0; i < blocks; i++) {\n",
        "        finalResult += h_output[i];\n",
        "    }\n",
        "\n",
        "    std::cout << \"Résultat final : \" << finalResult << std::endl;\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w988WcfWJGx",
        "outputId": "0f600bb3-198f-4737-aa59-60446b3318d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Tache2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o Tache2 Tache2.cu"
      ],
      "metadata": {
        "id": "2hE6iumQWRrh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./Tache2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf3gvx_4WSEo",
        "outputId": "74f32099-e29d-4e5a-f26a-5d8c0b43d478"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultat final : 524288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waYfPKvTWUhA",
        "outputId": "f3ca4e12-43dc-4715-ca41-d7d4fadcf16b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==9713== NVPROF is profiling process 9713, command: ./Tache2\n",
            "Résultat final : 524288\n",
            "==9713== Profiling application: ./Tache2\n",
            "==9713== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   95.20%  765.76us         1  765.76us  765.76us  765.76us  [CUDA memcpy HtoD]\n",
            "                    4.48%  36.064us         1  36.064us  36.064us  36.064us  reduceOptimized(float const *, float*, int)\n",
            "                    0.31%  2.5280us         1  2.5280us  2.5280us  2.5280us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.11%  191.53ms         2  95.764ms  77.585us  191.45ms  cudaMalloc\n",
            "                    0.49%  952.33us         2  476.17us  27.288us  925.04us  cudaMemcpy\n",
            "                    0.17%  335.42us         2  167.71us  134.59us  200.83us  cudaFree\n",
            "                    0.11%  208.05us         1  208.05us  208.05us  208.05us  cudaLaunchKernel\n",
            "                    0.09%  167.80us       114  1.4710us     137ns  83.408us  cuDeviceGetAttribute\n",
            "                    0.02%  37.881us         1  37.881us  37.881us  37.881us  cudaDeviceSynchronize\n",
            "                    0.01%  11.438us         1  11.438us  11.438us  11.438us  cuDeviceGetName\n",
            "                    0.00%  5.1060us         1  5.1060us  5.1060us  5.1060us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.3040us         1  4.3040us  4.3040us  4.3040us  cuDeviceTotalMem\n",
            "                    0.00%  2.6100us         2  1.3050us     183ns  2.4270us  cuDeviceGet\n",
            "                    0.00%  1.4420us         3     480ns     241ns     929ns  cuDeviceGetCount\n",
            "                    0.00%     679ns         1     679ns     679ns     679ns  cuModuleGetLoadingMode\n",
            "                    0.00%     358ns         1     358ns     358ns     358ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumé et explication\n",
        "\n",
        "Le kernel optimisé de réduction parallèle montre une efficacité notable avec seulement 36,064 µs consacrées à son exécution, représentant 4,48% du temps GPU. La majeure partie du temps GPU est cependant consacrée aux transferts de données entre l'hôte et le dispositif, avec 95,20% (765,76 µs) pour la copie HtoD et 0,31% (2,528 µs) pour la copie DtoH. Cela met en évidence que, comme souvent dans les applications CUDA, le transfert mémoire est un goulot d'étranglement.\n",
        "\n",
        "L'utilisation de la mémoire partagée et des instructions **__shfl_down_sync** permet une réduction efficace des données, exploitant la parallélisation intra-warp pour minimiser les étapes de synchronisation nécessaires. Le résultat final attendu (524288) confirme la précision de l'algorithme. Les appels API dominent également le temps total, en particulier cudaMalloc (99,11%), ce qui est typique pour l’allocation initiale de la mémoire GPU.\n",
        "\n",
        "L'optimisation des transferts mémoire, par exemple en regroupant davantage de calculs sur GPU avant de renvoyer les données, pourrait améliorer encore les performances globales de cette implémentation. Le kernel en lui-même est bien optimisé, mais les coûts de transfert masquent une partie de son efficacité."
      ],
      "metadata": {
        "id": "0Gjbt8tTWYSJ"
      }
    }
  ]
}